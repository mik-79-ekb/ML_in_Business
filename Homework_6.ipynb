{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание №6\n",
    "\n",
    "1. взять любой набор данных для бинарной классификации (можно скачать один из модельных с https://archive.ics.uci.edu/ml/datasets.php)\n",
    "3. сделать feature engineering\n",
    "4. обучить любой классификатор (какой вам нравится)\n",
    "5. далее разделить ваш набор данных на два множества: P (positives) и U (unlabeled). Причем брать нужно не все положительные (класс 1) примеры, а только лишь часть\n",
    "6. применить random negative sampling для построения классификатора в новых условиях\n",
    "7. сравнить качество с решением из пункта 4 (построить отчет - таблицу метрик)\n",
    "8. поэкспериментировать с долей P на шаге 5 (как будет меняться качество модели при уменьшении/увеличении размера P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник данных:\n",
    "https://www.machinelearningmastery.ru/standard-machine-learning-datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание данных:\n",
    "**Набор данных диабета индейцев Пима**\n",
    "\n",
    "Набор данных о диабете индейцев пима включает прогнозирование возникновения диабета в течение 5 лет у индейцев пима с учетом медицинских данных.\n",
    "\n",
    "Это бинарная (2-классная) задача классификации. Количество наблюдений для каждого класса не сбалансировано. Есть 768 наблюдений с 8 входными переменными и 1 выходной переменной. Считается, что пропущенные значения кодируются нулевыми значениями. Имена переменных следующие:\n",
    "\n",
    "    Количество раз беременных.\n",
    "    Концентрация глюкозы в плазме через 2 часа при оральном тесте на толерантность к глюкозе.\n",
    "    Диастолическое артериальное давление (мм рт. Ст.).\n",
    "    Толщина трехглавой кожной складки (мм).\n",
    "    2-часовой сывороточный инсулин (мю Ед / мл).\n",
    "    Индекс массы тела (вес в кг / (рост в м) ^ 2).\n",
    "    Родословная функция диабета.\n",
    "    Возраст (годы).\n",
    "    Переменная класса (0 или 1).\n",
    "\n",
    "Базовая эффективность прогнозирования наиболее распространенного класса - точность классификации приблизительно 65%. Лучшие результаты достигают точности классификации примерно 77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3  4     5      6   7  8\n",
       "0  6  148  72  35  0  33.6  0.627  50  1\n",
       "1  1   85  66  29  0  26.6  0.351  31  0\n",
       "2  8  183  64   0  0  23.3  0.672  32  1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('pima-indians-diabetes.data.csv', header=None)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть 8 признаков и 1 целевая переменная (бинарная) - нужно спрогнозировать возникновения диабета в течение 5 лет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего 768 наблюдений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       768 non-null    int64  \n",
      " 1   1       768 non-null    int64  \n",
      " 2   2       768 non-null    int64  \n",
      " 3   3       768 non-null    int64  \n",
      " 4   4       768 non-null    int64  \n",
      " 5   5       768 non-null    float64\n",
      " 6   6       768 non-null    float64\n",
      " 7   7       768 non-null    int64  \n",
      " 8   8       768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "датасет представлен числовыми наблюдениями (целыми и вещественными)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "пропусков нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на соотношение классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: 8, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, -1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "классы несколько разбалансированы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Создадим датафрейм результатов использования различных моделей:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_results = {\n",
    "    'Model name': [],\n",
    "    'f1': [],\n",
    "    'roc': [],\n",
    "    'recall': [],\n",
    "    'precision': []  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем выборку на тренировочную и тестовую части и обучаем модель (в примере - градиентный бустинг)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = data.iloc[:,:-1]\n",
    "y_data = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:14:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 61.25%\n",
      "roc: 69.98%\n",
      "recall: 58.33%\n",
      "precision: 64.47%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "def evaluate_results(y_test, y_predict):\n",
    "    print('Classification results:')\n",
    "    f1 = f1_score(y_test, y_predict)\n",
    "    models_results['f1'].append(f1)\n",
    "    print(\"f1: %.2f%%\" % (f1 * 100.0)) \n",
    "    roc = roc_auc_score(y_test, y_predict)\n",
    "    models_results['roc'].append(roc)\n",
    "    print(\"roc: %.2f%%\" % (roc * 100.0)) \n",
    "    rec = recall_score(y_test, y_predict, average='binary')\n",
    "    models_results['recall'].append(rec)\n",
    "    print(\"recall: %.2f%%\" % (rec * 100.0)) \n",
    "    prc = precision_score(y_test, y_predict, average='binary')\n",
    "    models_results['precision'].append(prc)\n",
    "    print(\"precision: %.2f%%\" % (prc * 100.0)) \n",
    "\n",
    "models_results['Model name'].append('xgboost')   \n",
    "evaluate_results(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "model_lgbm = lgbm.LGBMClassifier(random_state=21)\n",
    "model_lgbm.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model_lgbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 64.24%\n",
      "roc: 72.02%\n",
      "recall: 63.10%\n",
      "precision: 65.43%\n"
     ]
    }
   ],
   "source": [
    "models_results['Model name'].append('lightgbm')  \n",
    "evaluate_results(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as catb\n",
    "model_catb = catb.CatBoostClassifier(silent=True, random_state=21)\n",
    "model_catb.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model_catb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 69.51%\n",
      "roc: 76.11%\n",
      "recall: 67.86%\n",
      "precision: 71.25%\n"
     ]
    }
   ],
   "source": [
    "models_results['Model name'].append('catboost')  \n",
    "evaluate_results(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересно, но \"из коробки\" лючший результат у алгоритма catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь очередь за PU learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что нам неизвестны негативы и часть позитивов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 67/268 as positives and unlabeling the rest\n"
     ]
    }
   ],
   "source": [
    "mod_data = data.copy()\n",
    "#get the indices of the positives samples\n",
    "pos_ind = np.where(mod_data.iloc[:,-1].values == 1)[0]\n",
    "#shuffle them\n",
    "np.random.shuffle(pos_ind)\n",
    "# leave just 25% of the positives marked\n",
    "k = 0.25\n",
    "pos_sample_len = int(np.ceil(k * len(pos_ind)))\n",
    "print(f'Using {pos_sample_len}/{len(pos_ind)} as positives and unlabeling the rest')\n",
    "pos_sample = pos_ind[:pos_sample_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем столбец для новой целевой переменной, где у нас два класса - P (1) и U (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target variable:\n",
      " -1    701\n",
      " 1     67\n",
      "Name: class_test, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mod_data['class_test'] = -1\n",
    "mod_data.loc[pos_sample,'class_test'] = 1\n",
    "print('target variable:\\n', mod_data.iloc[:,-1].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>class_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1   2   3    4     5      6   7  8  class_test\n",
       "0   6  148  72  35    0  33.6  0.627  50  1           1\n",
       "1   1   85  66  29    0  26.6  0.351  31  0          -1\n",
       "2   8  183  64   0    0  23.3  0.672  32  1          -1\n",
       "3   1   89  66  23   94  28.1  0.167  21  0          -1\n",
       "4   0  137  40  35  168  43.1  2.288  33  1          -1\n",
       "5   5  116  74   0    0  25.6  0.201  30  0          -1\n",
       "6   3   78  50  32   88  31.0  0.248  26  1           1\n",
       "7  10  115   0   0    0  35.3  0.134  29  0          -1\n",
       "8   2  197  70  45  543  30.5  0.158  53  1          -1\n",
       "9   8  125  96   0    0   0.0  0.232  54  1          -1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = mod_data.iloc[:,:-2].values # just the X \n",
    "y_labeled = mod_data.iloc[:,-1].values # new class (just the P & U)\n",
    "y_positive = mod_data.iloc[:,-2].values # original class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. random negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 10) (67, 10)\n"
     ]
    }
   ],
   "source": [
    "mod_data = mod_data.sample(frac=1)\n",
    "neg_sample = mod_data[mod_data['class_test']==-1][:len(mod_data[mod_data['class_test']==1])]\n",
    "sample_test = mod_data[mod_data['class_test']==-1][len(mod_data[mod_data['class_test']==1]):]\n",
    "pos_sample = mod_data[mod_data['class_test']==1]\n",
    "print(neg_sample.shape, pos_sample.shape)\n",
    "sample_train = pd.concat([neg_sample, pos_sample]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:14:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 54.34%\n",
      "roc: 66.99%\n",
      "recall: 79.12%\n",
      "precision: 41.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "\n",
    "models_results['Model name'].append(f'PU learning_{k}_xgboost') \n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 55.20%\n",
      "roc: 67.88%\n",
      "recall: 80.22%\n",
      "precision: 42.07%\n"
     ]
    }
   ],
   "source": [
    "model_lgbm = lgbm.LGBMClassifier(random_state=21)\n",
    "model_lgbm.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "\n",
    "y_predict = model_lgbm.predict(sample_test.iloc[:,:-2].values)\n",
    "\n",
    "models_results['Model name'].append(f'PU learning_{k}_lightgbm') \n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 56.32%\n",
      "roc: 68.96%\n",
      "recall: 85.71%\n",
      "precision: 41.94%\n"
     ]
    }
   ],
   "source": [
    "model_catb = catb.CatBoostClassifier(silent=True, random_state=21)\n",
    "model_catb.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "\n",
    "y_predict = model_catb.predict(sample_test.iloc[:,:-2].values)\n",
    "\n",
    "models_results['Model name'].append(f'PU learning_{k}_catboost') \n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>catboost</td>\n",
       "      <td>0.695122</td>\n",
       "      <td>0.761054</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.642424</td>\n",
       "      <td>0.720238</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.654321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.699830</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.644737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PU learning_0.25_catboost</td>\n",
       "      <td>0.563177</td>\n",
       "      <td>0.689633</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PU learning_0.25_lightgbm</td>\n",
       "      <td>0.551985</td>\n",
       "      <td>0.678754</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.420749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PU learning_0.25_xgboost</td>\n",
       "      <td>0.543396</td>\n",
       "      <td>0.669941</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.413793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model name        f1       roc    recall  precision\n",
       "2                   catboost  0.695122  0.761054  0.678571   0.712500\n",
       "1                   lightgbm  0.642424  0.720238  0.630952   0.654321\n",
       "0                    xgboost  0.612500  0.699830  0.583333   0.644737\n",
       "5  PU learning_0.25_catboost  0.563177  0.689633  0.857143   0.419355\n",
       "4  PU learning_0.25_lightgbm  0.551985  0.678754  0.802198   0.420749\n",
       "3   PU learning_0.25_xgboost  0.543396  0.669941  0.791209   0.413793"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=models_results).sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментируем с долей P на шаге 5 (как будет меняться качество модели при уменьшении/увеличении размера P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [0.1, 0.3, 0.4, 0.5, 0.6, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 27/268 as positives and unlabeling the rest\n",
      "target variable:\n",
      " -1    741\n",
      " 1     27\n",
      "Name: class_test, dtype: int64\n",
      "(27, 10) (27, 10)\n",
      "[18:14:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 52.84%\n",
      "roc: 61.68%\n",
      "recall: 77.97%\n",
      "precision: 39.95%\n",
      "Classification results:\n",
      "f1: 53.80%\n",
      "roc: 62.50%\n",
      "recall: 81.06%\n",
      "precision: 40.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 57.95%\n",
      "roc: 66.83%\n",
      "recall: 90.75%\n",
      "precision: 42.56%\n",
      "Using 81/268 as positives and unlabeling the rest\n",
      "target variable:\n",
      " -1    687\n",
      " 1     81\n",
      "Name: class_test, dtype: int64\n",
      "(81, 10) (81, 10)\n",
      "[18:14:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 56.80%\n",
      "roc: 72.39%\n",
      "recall: 89.31%\n",
      "precision: 41.64%\n",
      "Classification results:\n",
      "f1: 56.29%\n",
      "roc: 71.86%\n",
      "recall: 88.68%\n",
      "precision: 41.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 57.03%\n",
      "roc: 72.83%\n",
      "recall: 93.08%\n",
      "precision: 41.11%\n",
      "Using 108/268 as positives and unlabeling the rest\n",
      "target variable:\n",
      " -1    660\n",
      " 1    108\n",
      "Name: class_test, dtype: int64\n",
      "(108, 10) (108, 10)\n",
      "[18:14:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 49.52%\n",
      "roc: 67.96%\n",
      "recall: 80.00%\n",
      "precision: 35.86%\n",
      "Classification results:\n",
      "f1: 49.65%\n",
      "roc: 68.14%\n",
      "recall: 81.54%\n",
      "precision: 35.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 52.71%\n",
      "roc: 71.39%\n",
      "recall: 86.15%\n",
      "precision: 37.97%\n",
      "Using 134/268 as positives and unlabeling the rest\n",
      "target variable:\n",
      " -1    634\n",
      " 1    134\n",
      "Name: class_test, dtype: int64\n",
      "(134, 10) (134, 10)\n",
      "[18:14:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 48.68%\n",
      "roc: 70.46%\n",
      "recall: 79.81%\n",
      "precision: 35.02%\n",
      "Classification results:\n",
      "f1: 49.26%\n",
      "roc: 70.96%\n",
      "recall: 79.81%\n",
      "precision: 35.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 51.92%\n",
      "roc: 73.75%\n",
      "recall: 84.62%\n",
      "precision: 37.45%\n",
      "Using 161/268 as positives and unlabeling the rest\n",
      "target variable:\n",
      " -1    607\n",
      " 1    161\n",
      "Name: class_test, dtype: int64\n",
      "(161, 10) (161, 10)\n",
      "[18:14:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 50.00%\n",
      "roc: 72.79%\n",
      "recall: 78.16%\n",
      "precision: 36.76%\n",
      "Classification results:\n",
      "f1: 51.47%\n",
      "roc: 74.21%\n",
      "recall: 80.46%\n",
      "precision: 37.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 48.75%\n",
      "roc: 71.81%\n",
      "recall: 78.16%\n",
      "precision: 35.42%\n",
      "Using 188/268 as positives and unlabeling the rest\n",
      "target variable:\n",
      " -1    580\n",
      " 1    188\n",
      "Name: class_test, dtype: int64\n",
      "(188, 10) (188, 10)\n",
      "[18:14:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 36.75%\n",
      "roc: 70.90%\n",
      "recall: 82.69%\n",
      "precision: 23.63%\n",
      "Classification results:\n",
      "f1: 38.30%\n",
      "roc: 72.98%\n",
      "recall: 86.54%\n",
      "precision: 24.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Mikhail\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 40.72%\n",
      "roc: 75.03%\n",
      "recall: 86.54%\n",
      "precision: 26.63%\n"
     ]
    }
   ],
   "source": [
    "for x in k:\n",
    "    mod_data = data.copy()\n",
    "    #get the indices of the positives samples\n",
    "    pos_ind = np.where(mod_data.iloc[:,-1].values == 1)[0]\n",
    "    #shuffle them\n",
    "    np.random.shuffle(pos_ind)\n",
    "    # leave just 25% of the positives marked\n",
    "    pos_sample_len = int(np.ceil(x * len(pos_ind)))\n",
    "    print(f'Using {pos_sample_len}/{len(pos_ind)} as positives and unlabeling the rest')\n",
    "    pos_sample = pos_ind[:pos_sample_len]\n",
    "    mod_data['class_test'] = -1\n",
    "    mod_data.loc[pos_sample,'class_test'] = 1\n",
    "    print('target variable:\\n', mod_data.iloc[:,-1].value_counts())\n",
    "    x_data = mod_data.iloc[:,:-2].values # just the X \n",
    "    y_labeled = mod_data.iloc[:,-1].values # new class (just the P & U)\n",
    "    y_positive = mod_data.iloc[:,-2].values # original class\n",
    "    mod_data = mod_data.sample(frac=1)\n",
    "    neg_sample = mod_data[mod_data['class_test']==-1][:len(mod_data[mod_data['class_test']==1])]\n",
    "    sample_test = mod_data[mod_data['class_test']==-1][len(mod_data[mod_data['class_test']==1]):]\n",
    "    pos_sample = mod_data[mod_data['class_test']==1]\n",
    "    print(neg_sample.shape, pos_sample.shape)\n",
    "    sample_train = pd.concat([neg_sample, pos_sample]).sample(frac=1)\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "    y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "    models_results['Model name'].append(f'PU learning_{x}_xgboost') \n",
    "    evaluate_results(sample_test.iloc[:,-2].values, y_predict)\n",
    "    model_lgbm = lgbm.LGBMClassifier(random_state=21)\n",
    "    model_lgbm.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "    y_predict = model_lgbm.predict(sample_test.iloc[:,:-2].values)\n",
    "    models_results['Model name'].append(f'PU learning_{x}_lightgbm') \n",
    "    evaluate_results(sample_test.iloc[:,-2].values, y_predict)\n",
    "    model_catb = catb.CatBoostClassifier(silent=True, random_state=21)\n",
    "    model_catb.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "    y_predict = model_catb.predict(sample_test.iloc[:,:-2].values)\n",
    "    models_results['Model name'].append(f'PU learning_{x}_catboost') \n",
    "    evaluate_results(sample_test.iloc[:,-2].values, y_predict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1\n",
      "  Model name        f1       roc    recall  precision\n",
      "2   catboost  0.695122  0.761054  0.678571   0.712500\n",
      "1   lightgbm  0.642424  0.720238  0.630952   0.654321\n",
      "0    xgboost  0.612500  0.699830  0.583333   0.644737\n",
      "roc\n",
      "                  Model name        f1       roc    recall  precision\n",
      "2                   catboost  0.695122  0.761054  0.678571   0.712500\n",
      "23  PU learning_0.7_catboost  0.407240  0.750339  0.865385   0.266272\n",
      "19  PU learning_0.6_lightgbm  0.514706  0.742132  0.804598   0.378378\n",
      "recall\n",
      "                  Model name        f1       roc    recall  precision\n",
      "11  PU learning_0.3_catboost  0.570328  0.728272  0.930818   0.411111\n",
      "8   PU learning_0.1_catboost  0.579466  0.668324  0.907489   0.425620\n",
      "9    PU learning_0.3_xgboost  0.568000  0.723946  0.893082   0.416422\n",
      "precision\n",
      "  Model name        f1       roc    recall  precision\n",
      "2   catboost  0.695122  0.761054  0.678571   0.712500\n",
      "1   lightgbm  0.642424  0.720238  0.630952   0.654321\n",
      "0    xgboost  0.612500  0.699830  0.583333   0.644737\n"
     ]
    }
   ],
   "source": [
    "for x in list(models_results.keys())[1:]:\n",
    "    print(x)\n",
    "    print(pd.DataFrame(data=models_results).sort_values(x, ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Бонусный вопрос:</b>\n",
    "\n",
    "Как вы думаете, какой из методов на практике является более предпочтительным: random negative sampling или 2-step approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваш ответ здесь:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Я думаю, что более популярен random negative sampling, главным образом благодаря тому, что проблема в итоге сводится к задаче бинарной классификации_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
